# FastLLM Windows 使用说明

## 目录结构

```
fastllm-win/
├── ftllm.exe              # 统一命令行入口 (添加此目录到 PATH)
├── bin/                   # 原生程序和运行时库
│   ├── *.exe              # 原生可执行文件
│   └── *.dll              # CUDA/VC++ 运行时库
├── ftllm/                 # Python 模块 (与上游 ftllm 包名一致)
├── docs/                  # 文档
└── web/                   # Web UI 静态资源
```

## 快速开始

### 1. 环境准备

1. 将解压后的根目录（ftllm.exe 所在目录）添加到系统 PATH 环境变量
2. 如需使用 `-py` 后端，需安装 Python 3.14+

### 2. 运行模型

```cmd
# 命令行聊天
ftllm chat D:\Models\Qwen3 --device cuda

# 启动 API 服务器
ftllm serve D:\Models\Qwen3 --port 8080 --device cuda

# 启动 Web UI
ftllm webui D:\Models\Qwen3 --port 1616 --device cuda

# 使用 Python 后端 (支持更多功能如 LoRA)
ftllm -py chat D:\Models\Qwen3 --device cuda
```

---

## ftllm 命令参考

### 命令列表

```
Usage: ftllm <command> [options] [model_path]

命令:
  chat, run              命令行聊天
  serve, server          启动 OpenAI 兼容 API 服务器
  webui                  启动 Web UI
  bench, benchmark       性能测试
  quant, quantize        模型量化
  download               下载 HuggingFace 模型 (需 -py)
  export                 导出/转换模型 (需 -py)
  config                 配置管理 (需 -py)

选项:
  -py                    使用 Python 后端 (支持 LoRA 等高级功能)
  -h, --help             显示帮助
  -v, --version          显示版本
```

### 通用参数

```
模型路径:
  <model>                模型路径 (位置参数)
  -p, --path <路径>      模型路径，fastllm模型文件或HF模型文件夹

设备配置:
  --device <设备>        使用的设备: cuda, cpu, numa, tfacc
  --moe_device <设备>    MOE层使用的设备 (MOE模型专用)
  --moe_experts <数量>   MOE使用的专家数

性能参数:
  -t, --threads <数量>   CPU线程数量 (默认: 自动)
  -l, --low              使用低内存模式
  --max_batch <数量>     每次最多同时推理的询问数量

数据类型:
  --dtype <类型>         权重类型 (读取HF模型时有效)
                         可选: auto, float16, int8, int4, int4g128, int4g256, fp8_e4m3
  --moe_dtype <类型>     MOE层权重类型
  --atype <类型>         推理类型: auto, float32, float16

CUDA 选项:
  --cuda_embedding       在CUDA上进行embedding (需要更多显存)
  --kv_cache_limit <值>  KV缓存最大使用量 (如: 5G, 100M, 168K, auto)
  --cuda_shared_expert   是否使用CUDA执行共享专家 (默认: true)

缓存与优化:
  --cache_history <值>   缓存历史对话 (true/false)
  --cache_fast <值>      启用快速缓存，消耗更多显存 (true/false)
  --cache_dir <路径>     指定缓存模型文件的路径

高级选项:
  --enable_thinking <值> 开启硬思考开关，需要模型支持 (true/false)
  --enable_amx <值>      开启AMX加速 (true/false)
  --lora <路径>          指定LoRA路径
  --custom <文件>        指定自定义模型的Python文件
  --dtype_config <文件>  指定权重类型配置文件
  --ori <路径>           原始模型权重路径 (读取GGUF时使用)

对话模板:
  --tool_call_parser <类型>  使用的tool_call_parser类型 (默认: auto)
  --chat_template <文件>     使用的chat_template文件 (Jinja2格式)
```

### API Server 参数 (ftllm serve)

```
  --model_name <名称>    部署的模型名称 (API调用时核验)
  --host <地址>          API服务器主机地址 (默认: 127.0.0.1)
  --port <端口>          API服务器端口号 (默认: 8080)
  --api_key <密钥>       API Key认证
  --think <值>           处理丢失的<think>标签 (true/false)
  --hide_input           不显示请求信息
  --dev_mode             开发模式，可获取对话列表并主动停止
```

说明：
- `--think` 控制的是"输出是否带 <think> 标签"。

### Web UI 参数 (ftllm webui)

```
  --port <端口>          WebUI端口号 (默认: 1616)
  --max_token <数量>     输出最大token数 (默认: 4096)
  --think <值>           处理丢失的<think>标签 (true/false)
```

---

## 使用示例

### 基础对话

```cmd
ftllm chat D:\Models\Qwen2-7B-Instruct --device cuda

# 使用 Python 后端 (支持 LoRA 等)
ftllm -py chat D:\Models\Qwen2-7B-Instruct --device cuda
```

### 量化加载

```cmd
# INT8 量化
ftllm run D:\Models\Qwen2-7B-Instruct --dtype int8 --device cuda

# INT4 分组量化 (128组)
ftllm run D:\Models\Qwen2-7B-Instruct --dtype int4g128 --device cuda
```

### MOE 模型 (如 DeepSeek-V3, Qwen3-MoE)

```cmd
# GPU推理Dense层 + CPU推理MOE层
ftllm run D:\Models\DeepSeek-V3 --device cuda --moe_device cpu

# 指定MOE专家数
ftllm run D:\Models\DeepSeek-V3 --device cuda --moe_device cpu --moe_experts 8
```

### 启动 API 服务器

```cmd
# 需要安装依赖: pip install fastapi uvicorn
ftllm serve D:\Models\Qwen2-7B-Instruct --port 8080 --device cuda
```

API 调用示例:
```python
from openai import OpenAI
client = OpenAI(base_url="http://localhost:8080/v1", api_key="none")
response = client.chat.completions.create(
    model="default",
    messages=[{"role": "user", "content": "你好"}]
)
print(response.choices[0].message.content)
```

### 启动 Web UI

```cmd
# 需要安装依赖: pip install streamlit
ftllm webui D:\Models\Qwen2-7B-Instruct --port 1616 --device cuda
```

### 使用配置文件

创建 `config.json`:
```json
{
    "model": "D:/Models/Qwen2-7B-Instruct",
    "device": "cuda",
    "dtype": "int4g128",
    "threads": 8,
    "kv_cache_limit": "8G"
}
```

运行:
```cmd
ftllm run config.json
```

### 限制显存/KV缓存

```cmd
# 限制KV缓存为5GB
ftllm run D:\Models\Qwen2-7B-Instruct --device cuda --kv_cache_limit 5G

# 低内存模式
ftllm run D:\Models\Qwen2-7B-Instruct --device cuda -l
```

---

## 可选依赖安装

基础的 `ftllm run` / `ftllm chat` 命令无需安装任何额外依赖。

如需使用高级功能，运行 `install-deps.cmd` 或手动安装:

```cmd
# API Server
pip install fastapi pydantic uvicorn shortuuid openai

# Web UI
pip install streamlit

# HuggingFace 下载
pip install huggingface_hub

# 全部安装
pip install -r ftllm\requirements.txt
```

---

## 支持的模型

- Qwen / Qwen2 / Qwen3 / Qwen3-MoE / Qwen3-Next
- ChatGLM / GLM4 / GLM4-MoE
- Llama / Llama2 / Llama3
- DeepSeek / DeepSeek-V2 / DeepSeek-V3
- MiniCPM / MiniCPM3
- InternLM2
- Phi3
- BERT / XLM-RoBERTa
- MiniMax
- HunYuan-MoE
- Ernie4.5-MoE
- Pangu-MoE
- 更多...

---

## 常见问题

### Q: 找不到 Python
确保 Python 已安装并添加到 PATH 环境变量。

### Q: DLL 加载失败
确保根目录（ftllm.exe 所在目录）已添加到 PATH 环境变量。

### Q: 显存不足
- 使用 `--dtype int4g128` 降低权重精度
- 使用 `--kv_cache_limit` 限制缓存大小
- 对于 MOE 模型使用 `--moe_device cpu`

### Q: 速度慢
- 确保使用 `--device cuda` 启用 GPU
- 增加 `--threads` 参数 (CPU模式)
- 使用 `--cuda_embedding` (显存充足时)

---

## 构建说明

使用 `build.ps1` 脚本进行构建:

```powershell
# 交互模式
.\build.ps1

# 自动模式 - CPU版本
.\build.ps1 -Auto -Target cpu

# 自动模式 - CUDA版本 (本机架构)
.\build.ps1 -Auto -Target cuda -CudaArch native

# 自动模式 - CUDA版本 (全架构) + 清理重建 + 打包
.\build.ps1 -Auto -Target cuda -CudaArch all -Clean -Package

# 同时构建 CPU 和 CUDA 版本
.\build.ps1 -Auto -Target both
```

构建参数:
- `-Auto`: 自动模式，使用命令行参数
- `-Target`: 构建目标 (cpu/cuda/both)
- `-CudaArch`: CUDA架构 (native/all/75/80/86/89/90/120)
- `-Clean`: 清理重建
- `-Package`: 构建后自动打包
