# FastLLM Windows 使用说明

## 目录结构

```
fastllm-win/
├── bin/                    # 可执行文件目录
│   ├── ftllm.exe          # 统一命令行入口 (推荐)
│   ├── apiserver.exe      # 原生 API 服务器
│   ├── webui.exe          # 原生 Web UI 服务器
│   ├── FastllmStudio_cli.exe # 原生命令行聊天程序
│   ├── benchmark.exe      # 性能测试工具
│   ├── quant.exe          # 模型量化工具
│   ├── install-deps.cmd   # 可选依赖安装脚本
│   ├── cublas64_*.dll     # CUDA 运行时库 (CUDA 版本)
│   ├── cublasLt64_*.dll
│   ├── cudart64_*.dll
│   └── vcruntime*.dll     # VC++ 运行时库
├── pytools/                # Python 模块目录
│   ├── pyfastllm2.pyd     # Python 绑定模块
│   ├── requirements.txt   # 可选 Python 依赖
│   └── *.py               # Python 源文件
├── docs/                   # 文档目录
└── web/                    # Web UI 静态资源
```

## 快速开始

### 1. 环境准备

1. 将 `bin` 目录添加到系统 PATH 环境变量
2. 确保已安装 Python 3.8+ (推荐 3.10+)

### 2. 运行模型

**方式一：统一入口（推荐）**
```cmd
# 默认使用 C++ 原生程序
ftllm chat D:\Models\Qwen3 --device cuda

# 使用 Python 后端 (支持更多功能)
ftllm -py chat D:\Models\Qwen3 --device cuda
```

**方式二：直接调用原生程序**
```cmd
FastllmStudio_cli.exe -p D:\Models\Qwen3 --device cuda
```

---

## Python CLI 命令 (ftllm)

### 子命令列表

| 命令 | 说明 | 额外依赖 |
|------|------|----------|
| `ftllm run <模型>` | 运行模型进行对话 | 无 |
| `ftllm chat <模型>` | 运行模型进行对话 | 无 |
| `ftllm serve <模型>` | 启动 OpenAI 兼容 API 服务 | fastapi, uvicorn |
| `ftllm webui <模型>` | 启动 Web UI | streamlit |
| `ftllm export` | 导出/转换模型 | 无 |
| `ftllm download <模型>` | 从 HuggingFace 下载模型 | huggingface_hub |
| `ftllm config` | 创建配置文件 | 无 |
| `ftllm -v` | 显示版本 | 无 |

### 通用参数

```
模型路径:
  <model>                模型路径 (位置参数)
  -p, --path <路径>      模型路径，fastllm模型文件或HF模型文件夹

设备配置:
  --device <设备>        使用的设备: cuda, cpu, numa, tfacc
  --moe_device <设备>    MOE层使用的设备 (MOE模型专用)
  --moe_experts <数量>   MOE使用的专家数

性能参数:
  -t, --threads <数量>   CPU线程数量 (默认: 自动)
  -l, --low              使用低内存模式
  --max_batch <数量>     每次最多同时推理的询问数量

数据类型:
  --dtype <类型>         权重类型 (读取HF模型时有效)
                         可选: auto, float16, int8, int4, int4g128, int4g256, fp8_e4m3
  --moe_dtype <类型>     MOE层权重类型
  --atype <类型>         推理类型: auto, float32, float16

CUDA 选项:
  --cuda_embedding       在CUDA上进行embedding (需要更多显存)
  --kv_cache_limit <值>  KV缓存最大使用量 (如: 5G, 100M, 168K, auto)
  --cuda_shared_expert   是否使用CUDA执行共享专家 (默认: true)

缓存与优化:
  --cache_history <值>   缓存历史对话 (true/false)
  --cache_fast <值>      启用快速缓存，消耗更多显存 (true/false)
  --cache_dir <路径>     指定缓存模型文件的路径

高级选项:
  --enable_thinking <值> 开启硬思考开关，需要模型支持 (true/false)
  --enable_amx <值>      开启AMX加速 (true/false)
  --lora <路径>          指定LoRA路径
  --custom <文件>        指定自定义模型的Python文件
  --dtype_config <文件>  指定权重类型配置文件
  --ori <路径>           原始模型权重路径 (读取GGUF时使用)

对话模板:
  --tool_call_parser <类型>  使用的tool_call_parser类型 (默认: auto)
  --chat_template <文件>     使用的chat_template文件 (Jinja2格式)
```

### API Server 参数 (ftllm serve)

```
  --model_name <名称>    部署的模型名称 (API调用时核验)
  --host <地址>          API服务器主机地址 (默认: 0.0.0.0)
  --port <端口>          API服务器端口号 (默认: 8080)
  --api_key <密钥>       API Key认证
  --think <值>           处理丢失的<think>标签 (true/false)
  --hide_input           不显示请求信息
  --dev_mode             开发模式，可获取对话列表并主动停止
```

### Web UI 参数 (ftllm webui)

```
  --port <端口>          WebUI端口号 (默认: 1616)
  --max_token <数量>     输出最大token数 (默认: 4096)
  --think <值>           处理丢失的<think>标签 (true/false)
```

---

## 原生程序参数

### ftllm.exe - 统一命令行入口

```
Usage: ftllm <command> [options] [model_path]

命令 (默认使用 C++ 原生程序):
  serve, server, api     启动 API 服务器 → apiserver.exe
  webui, web             启动 Web UI → webui.exe
  chat, run              命令行聊天 → FastllmStudio_cli.exe
  bench, benchmark       性能测试 → benchmark.exe
  quant, quantize        模型量化 → quant.exe

Python 专用命令 (自动使用 Python 后端):
  download               下载 HuggingFace 模型
  ui                     启动 Streamlit UI
  config                 配置管理
  export                 导出/转换模型

选项:
  -py                    强制使用 Python 后端
  -h, --help             显示帮助
  -v, --version          显示版本
```

### FastllmStudio_cli.exe - 命令行聊天

```
  -h, --help             显示帮助
  -p, --path <路径>      模型文件路径
  -t, --threads <数量>   使用的线程数量
  -l, --low              使用低内存模式
  --system <提示词>      设置系统提示词
  --eos_token <token>    设置eos token
  --dtype <类型>         设置权重类型 (读取HF文件时生效)
  --atype <类型>         设置推理数据类型 (float32/float16)
  --top_p <值>           采样参数 top_p
  --top_k <值>           采样参数 top_k
  --temperature <值>     采样参数温度，越高结果越随机
  --repeat_penalty <值>  采样参数重复惩罚
```

### apiserver.exe - API 服务器

```
  -p, --path <路径>      模型文件路径
  --host <地址>          服务器地址 (默认: 0.0.0.0)
  --port <端口>          服务器端口 (默认: 8080)
  --threads <数量>       线程数量
  --dtype <类型>         权重类型
```

### webui.exe - Web UI 服务器

```
  -p, --path <路径>      模型文件路径
  --port <端口>          服务器端口 (默认: 8080)
  --threads <数量>       线程数量
  --dtype <类型>         权重类型
```

### quant.exe - 模型量化

```
  <输入路径>             输入模型路径
  <输出路径>             输出模型路径
  <量化位数>             量化位数: 8 或 4
```

### benchmark.exe - 性能测试

```
  -p, --path <路径>      模型文件路径
  --threads <数量>       线程数量
  --device <设备>        设备类型
```

---

## 使用示例

### 基础对话

```cmd
# 统一入口 (推荐)
ftllm chat D:\Models\Qwen2-7B-Instruct --device cuda

# 使用 Python 后端
ftllm -py chat D:\Models\Qwen2-7B-Instruct --device cuda

# 直接调用原生程序
FastllmStudio_cli.exe -p D:\Models\Qwen2-7B-Instruct --device cuda
```

### 量化加载

```cmd
# INT8 量化
ftllm run D:\Models\Qwen2-7B-Instruct --dtype int8 --device cuda

# INT4 分组量化 (128组)
ftllm run D:\Models\Qwen2-7B-Instruct --dtype int4g128 --device cuda
```

### MOE 模型 (如 DeepSeek-V3, Qwen3-MoE)

```cmd
# GPU推理Dense层 + CPU推理MOE层
ftllm run D:\Models\DeepSeek-V3 --device cuda --moe_device cpu

# 指定MOE专家数
ftllm run D:\Models\DeepSeek-V3 --device cuda --moe_device cpu --moe_experts 8
```

### 启动 API 服务器

```cmd
# Python 版本 (需要安装依赖: pip install fastapi uvicorn)
ftllm serve D:\Models\Qwen2-7B-Instruct --port 8080 --device cuda

# 原生版本
apiserver.exe -p D:\Models\Qwen2-7B-Instruct --port 8080
```

API 调用示例:
```python
from openai import OpenAI
client = OpenAI(base_url="http://localhost:8080/v1", api_key="none")
response = client.chat.completions.create(
    model="default",
    messages=[{"role": "user", "content": "你好"}]
)
print(response.choices[0].message.content)
```

### 启动 Web UI

```cmd
# Python 版本 (需要安装依赖: pip install streamlit)
ftllm webui D:\Models\Qwen2-7B-Instruct --port 1616 --device cuda

# 原生版本
webui.exe -p D:\Models\Qwen2-7B-Instruct --port 8080
```

### 使用配置文件

创建 `config.json`:
```json
{
    "model": "D:/Models/Qwen2-7B-Instruct",
    "device": "cuda",
    "dtype": "int4g128",
    "threads": 8,
    "kv_cache_limit": "8G"
}
```

运行:
```cmd
ftllm run config.json
```

### 限制显存/KV缓存

```cmd
# 限制KV缓存为5GB
ftllm run D:\Models\Qwen2-7B-Instruct --device cuda --kv_cache_limit 5G

# 低内存模式
ftllm run D:\Models\Qwen2-7B-Instruct --device cuda -l
```

---

## 可选依赖安装

基础的 `ftllm run` / `ftllm chat` 命令无需安装任何额外依赖。

如需使用高级功能，运行 `install-deps.cmd` 或手动安装:

```cmd
# API Server
pip install fastapi pydantic uvicorn shortuuid openai

# Web UI
pip install streamlit

# HuggingFace 下载
pip install huggingface_hub

# 全部安装
pip install -r ftllm\requirements.txt
```

---

## 支持的模型

- Qwen / Qwen2 / Qwen3 / Qwen3-MoE / Qwen3-Next
- ChatGLM / GLM4 / GLM4-MoE
- Llama / Llama2 / Llama3
- DeepSeek / DeepSeek-V2 / DeepSeek-V3
- MiniCPM / MiniCPM3
- InternLM2
- Phi3
- BERT / XLM-RoBERTa
- MiniMax
- HunYuan-MoE
- Ernie4.5-MoE
- Pangu-MoE
- 更多...

---

## 常见问题

### Q: 找不到 Python
确保 Python 已安装并添加到 PATH 环境变量。

### Q: DLL 加载失败
确保 `bin` 目录在 PATH 中，或将 CUDA/VC++ DLL 复制到程序同目录。

### Q: 显存不足
- 使用 `--dtype int4g128` 降低权重精度
- 使用 `--kv_cache_limit` 限制缓存大小
- 对于 MOE 模型使用 `--moe_device cpu`

### Q: 速度慢
- 确保使用 `--device cuda` 启用 GPU
- 增加 `--threads` 参数 (CPU模式)
- 使用 `--cuda_embedding` (显存充足时)

---

## 构建说明

使用 `build.ps1` 脚本进行构建:

```powershell
# 交互模式
.\build.ps1

# 自动模式 - CPU版本
.\build.ps1 -Auto -Target cpu

# 自动模式 - CUDA版本 (本机架构)
.\build.ps1 -Auto -Target cuda -CudaArch native

# 自动模式 - CUDA版本 (全架构) + 清理重建 + 打包
.\build.ps1 -Auto -Target cuda -CudaArch all -Clean -Package

# 同时构建 CPU 和 CUDA 版本
.\build.ps1 -Auto -Target both
```

构建参数:
- `-Auto`: 自动模式，使用命令行参数
- `-Target`: 构建目标 (cpu/cuda/both)
- `-CudaArch`: CUDA架构 (native/all/75/80/86/89/90/120)
- `-Clean`: 清理重建
- `-Package`: 构建后自动打包
